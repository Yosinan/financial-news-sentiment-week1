{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 1: Exploratory Data Analysis (EDA)\n",
        "\n",
        "**Financial News Sentiment Analysis - Week 1 Challenge**\n",
        "\n",
        "This notebook performs comprehensive EDA on the financial news dataset to understand:\n",
        "- Descriptive statistics (headline length, articles per publisher, publication dates)\n",
        "- Text analysis and topic modeling (NLP keywords/phrases)\n",
        "- Time series analysis (publication frequency over time)\n",
        "- Publisher analysis (most active publishers, domain analysis)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Data Loading\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard library imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "import re\n",
        "\n",
        "# NLP imports\n",
        "from collections import Counter\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download required NLTK data\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt', quiet=True)\n",
        "\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords', quiet=True)\n",
        "\n",
        "try:\n",
        "    nltk.data.find('corpora/wordnet')\n",
        "except LookupError:\n",
        "    nltk.download('wordnet', quiet=True)\n",
        "\n",
        "# Set style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set paths\n",
        "PROJECT_ROOT = Path('..')\n",
        "DATA_DIR = PROJECT_ROOT / 'data'\n",
        "FIGURES_DIR = PROJECT_ROOT / 'figures'\n",
        "\n",
        "# Create directories if they don't exist\n",
        "FIGURES_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "print(\"Setup complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the financial news dataset\n",
        "# Note: Update the filename based on your actual data file\n",
        "data_files = list(DATA_DIR.glob('*.csv')) + list(DATA_DIR.glob('*.json'))\n",
        "\n",
        "if not data_files:\n",
        "    print(\"âš ï¸  No data files found in data/ directory.\")\n",
        "    print(\"Please ensure your dataset is placed in the data/ directory.\")\n",
        "    print(\"Expected columns: headline, url, publisher, date, stock\")\n",
        "else:\n",
        "    # Try to load the first data file found\n",
        "    data_file = data_files[0]\n",
        "    print(f\"Loading data from: {data_file.name}\")\n",
        "    \n",
        "    if data_file.suffix == '.csv':\n",
        "        df = pd.read_csv(data_file, parse_dates=['date'], low_memory=False)\n",
        "    elif data_file.suffix == '.json':\n",
        "        df = pd.read_json(data_file, convert_dates=['date'])\n",
        "    \n",
        "    print(f\"\\nâœ… Data loaded successfully!\")\n",
        "    print(f\"Shape: {df.shape[0]:,} rows Ã— {df.shape[1]} columns\")\n",
        "    print(f\"\\nFirst few rows:\")\n",
        "    display(df.head())\n",
        "    print(f\"\\nColumn names: {list(df.columns)}\")\n",
        "    print(f\"\\nData types:\")\n",
        "    display(df.dtypes)\n",
        "    print(f\"\\nBasic info:\")\n",
        "    display(df.info())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Cleaning and Preparation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for missing values\n",
        "print(\"Missing values per column:\")\n",
        "missing = df.isnull().sum()\n",
        "missing_pct = (missing / len(df) * 100).round(2)\n",
        "missing_df = pd.DataFrame({\n",
        "    'Missing Count': missing,\n",
        "    'Percentage': missing_pct\n",
        "})\n",
        "display(missing_df[missing_df['Missing Count'] > 0])\n",
        "\n",
        "# Handle missing values if needed\n",
        "# Drop rows with missing headlines (critical for analysis)\n",
        "initial_count = len(df)\n",
        "df = df.dropna(subset=['headline'])\n",
        "print(f\"\\nDropped {initial_count - len(df)} rows with missing headlines\")\n",
        "print(f\"Remaining rows: {len(df):,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ensure date column is datetime\n",
        "if 'date' in df.columns:\n",
        "    df['date'] = pd.to_datetime(df['date'], errors='coerce', utc=True)\n",
        "    # Convert from UTC-4 to UTC if needed (adjust based on your data)\n",
        "    # df['date'] = df['date'] + pd.Timedelta(hours=4)\n",
        "    \n",
        "    # Extract date components\n",
        "    df['year'] = df['date'].dt.year\n",
        "    df['month'] = df['date'].dt.month\n",
        "    df['day'] = df['date'].dt.day\n",
        "    df['day_of_week'] = df['date'].dt.day_name()\n",
        "    df['hour'] = df['date'].dt.hour\n",
        "    \n",
        "    print(\"Date column processed successfully!\")\n",
        "    print(f\"Date range: {df['date'].min()} to {df['date'].max()}\")\n",
        "\n",
        "# Clean headline text\n",
        "if 'headline' in df.columns:\n",
        "    df['headline_length'] = df['headline'].str.len()\n",
        "    df['headline_word_count'] = df['headline'].str.split().str.len()\n",
        "    print(\"\\nHeadline statistics calculated!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Descriptive Statistics\n",
        "\n",
        "### 3.1 Headline Length Statistics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic statistics for headline lengths\n",
        "print(\"=== Headline Length Statistics ===\")\n",
        "headline_stats = df['headline_length'].describe()\n",
        "display(headline_stats)\n",
        "\n",
        "print(\"\\n=== Headline Word Count Statistics ===\")\n",
        "word_count_stats = df['headline_word_count'].describe()\n",
        "display(word_count_stats)\n",
        "\n",
        "# Visualizations\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Headline length distribution\n",
        "axes[0].hist(df['headline_length'], bins=50, edgecolor='black', alpha=0.7)\n",
        "axes[0].axvline(df['headline_length'].mean(), color='red', linestyle='--', \n",
        "                label=f'Mean: {df[\"headline_length\"].mean():.1f}')\n",
        "axes[0].axvline(df['headline_length'].median(), color='green', linestyle='--', \n",
        "                label=f'Median: {df[\"headline_length\"].median():.1f}')\n",
        "axes[0].set_xlabel('Headline Length (characters)')\n",
        "axes[0].set_ylabel('Frequency')\n",
        "axes[0].set_title('Distribution of Headline Lengths')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Word count distribution\n",
        "axes[1].hist(df['headline_word_count'], bins=30, edgecolor='black', alpha=0.7, color='orange')\n",
        "axes[1].axvline(df['headline_word_count'].mean(), color='red', linestyle='--', \n",
        "                label=f'Mean: {df[\"headline_word_count\"].mean():.1f}')\n",
        "axes[1].axvline(df['headline_word_count'].median(), color='green', linestyle='--', \n",
        "                label=f'Median: {df[\"headline_word_count\"].median():.1f}')\n",
        "axes[1].set_xlabel('Word Count')\n",
        "axes[1].set_ylabel('Frequency')\n",
        "axes[1].set_title('Distribution of Headline Word Counts')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIGURES_DIR / 'headline_length_distributions.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nâœ… Headline length analysis complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Count articles per publisher\n",
        "publisher_counts = df['publisher'].value_counts()\n",
        "publisher_stats = pd.DataFrame({\n",
        "    'Article Count': publisher_counts,\n",
        "    'Percentage': (publisher_counts / len(df) * 100).round(2)\n",
        "})\n",
        "\n",
        "print(f\"=== Publisher Statistics ===\")\n",
        "print(f\"Total unique publishers: {df['publisher'].nunique()}\")\n",
        "print(f\"\\nTop 20 Publishers:\")\n",
        "display(publisher_stats.head(20))\n",
        "\n",
        "# Visualization\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Top 15 publishers bar chart\n",
        "top_publishers = publisher_counts.head(15)\n",
        "axes[0].barh(range(len(top_publishers)), top_publishers.values, color='steelblue')\n",
        "axes[0].set_yticks(range(len(top_publishers)))\n",
        "axes[0].set_yticklabels(top_publishers.index, fontsize=9)\n",
        "axes[0].set_xlabel('Number of Articles')\n",
        "axes[0].set_title('Top 15 Publishers by Article Count')\n",
        "axes[0].invert_yaxis()\n",
        "axes[0].grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "# Add value labels\n",
        "for i, v in enumerate(top_publishers.values):\n",
        "    axes[0].text(v + max(top_publishers.values) * 0.01, i, f'{v:,}', \n",
        "                va='center', fontsize=8)\n",
        "\n",
        "# Distribution of articles per publisher (log scale)\n",
        "axes[1].hist(publisher_counts.values, bins=50, edgecolor='black', alpha=0.7, color='coral')\n",
        "axes[1].set_xlabel('Number of Articles per Publisher')\n",
        "axes[1].set_ylabel('Number of Publishers')\n",
        "axes[1].set_title('Distribution of Articles per Publisher')\n",
        "axes[1].set_yscale('log')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIGURES_DIR / 'publisher_analysis.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nâœ… Publisher analysis complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 Publication Date Trends\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze publication dates\n",
        "print(\"=== Publication Date Analysis ===\")\n",
        "print(f\"Date range: {df['date'].min()} to {df['date'].max()}\")\n",
        "print(f\"Total days covered: {(df['date'].max() - df['date'].min()).days} days\")\n",
        "\n",
        "# Articles by year\n",
        "yearly_counts = df['year'].value_counts().sort_index()\n",
        "print(\"\\nArticles by Year:\")\n",
        "display(yearly_counts)\n",
        "\n",
        "# Articles by month\n",
        "monthly_counts = df['month'].value_counts().sort_index()\n",
        "month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', \n",
        "               'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
        "print(\"\\nArticles by Month:\")\n",
        "monthly_df = pd.DataFrame({\n",
        "    'Month': [month_names[m-1] for m in monthly_counts.index],\n",
        "    'Count': monthly_counts.values\n",
        "})\n",
        "display(monthly_df)\n",
        "\n",
        "# Articles by day of week\n",
        "dow_counts = df['day_of_week'].value_counts()\n",
        "dow_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
        "dow_counts = dow_counts.reindex([d for d in dow_order if d in dow_counts.index])\n",
        "print(\"\\nArticles by Day of Week:\")\n",
        "display(dow_counts)\n",
        "\n",
        "# Articles by hour\n",
        "hourly_counts = df['hour'].value_counts().sort_index()\n",
        "print(\"\\nTop 10 Hours with Most Articles:\")\n",
        "display(hourly_counts.head(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Time series visualizations\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# Daily article count over time\n",
        "daily_counts = df.groupby(df['date'].dt.date).size()\n",
        "axes[0, 0].plot(daily_counts.index, daily_counts.values, linewidth=1.5, color='steelblue')\n",
        "axes[0, 0].set_xlabel('Date')\n",
        "axes[0, 0].set_ylabel('Number of Articles')\n",
        "axes[0, 0].set_title('Daily Article Publication Frequency Over Time')\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "axes[0, 0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Articles by month\n",
        "axes[0, 1].bar(range(len(monthly_counts)), monthly_counts.values, color='coral')\n",
        "axes[0, 1].set_xticks(range(len(monthly_counts)))\n",
        "axes[0, 1].set_xticklabels([month_names[m-1] for m in monthly_counts.index], rotation=45)\n",
        "axes[0, 1].set_ylabel('Number of Articles')\n",
        "axes[0, 1].set_title('Articles by Month')\n",
        "axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Articles by day of week\n",
        "axes[1, 0].bar(range(len(dow_counts)), dow_counts.values, color='mediumseagreen')\n",
        "axes[1, 0].set_xticks(range(len(dow_counts)))\n",
        "axes[1, 0].set_xticklabels(dow_counts.index, rotation=45)\n",
        "axes[1, 0].set_ylabel('Number of Articles')\n",
        "axes[1, 0].set_title('Articles by Day of Week')\n",
        "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Articles by hour of day\n",
        "axes[1, 1].bar(hourly_counts.index, hourly_counts.values, color='gold', alpha=0.7)\n",
        "axes[1, 1].set_xlabel('Hour of Day (UTC)')\n",
        "axes[1, 1].set_ylabel('Number of Articles')\n",
        "axes[1, 1].set_title('Articles by Hour of Day')\n",
        "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
        "axes[1, 1].set_xticks(range(0, 24, 2))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIGURES_DIR / 'publication_time_analysis.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nâœ… Publication date analysis complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Text Analysis and Topic Modeling\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize NLP tools\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Clean and preprocess text for analysis\"\"\"\n",
        "    if pd.isna(text):\n",
        "        return []\n",
        "    \n",
        "    # Convert to lowercase\n",
        "    text = str(text).lower()\n",
        "    \n",
        "    # Remove special characters but keep spaces\n",
        "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
        "    \n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(text)\n",
        "    \n",
        "    # Remove stopwords and short words\n",
        "    tokens = [word for word in tokens if word not in stop_words and len(word) > 2]\n",
        "    \n",
        "    # Lemmatize\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "    \n",
        "    return tokens\n",
        "\n",
        "print(\"Text preprocessing function ready!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Process all headlines\n",
        "print(\"Processing headlines for text analysis...\")\n",
        "df['processed_tokens'] = df['headline'].apply(preprocess_text)\n",
        "\n",
        "# Flatten all tokens\n",
        "all_tokens = []\n",
        "for tokens in df['processed_tokens']:\n",
        "    all_tokens.extend(tokens)\n",
        "\n",
        "# Count word frequencies\n",
        "word_freq = Counter(all_tokens)\n",
        "top_words = word_freq.most_common(50)\n",
        "\n",
        "print(f\"\\nTotal unique words: {len(word_freq)}\")\n",
        "print(f\"Total word occurrences: {len(all_tokens):,}\")\n",
        "print(f\"\\nTop 50 Most Common Words:\")\n",
        "for word, count in top_words:\n",
        "    print(f\"  {word}: {count:,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize top words\n",
        "top_30_words = top_words[:30]\n",
        "words, counts = zip(*top_30_words)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "ax.barh(range(len(words)), counts, color='steelblue')\n",
        "ax.set_yticks(range(len(words)))\n",
        "ax.set_yticklabels(words)\n",
        "ax.set_xlabel('Frequency')\n",
        "ax.set_title('Top 30 Most Common Words in Headlines')\n",
        "ax.invert_yaxis()\n",
        "ax.grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "# Add value labels\n",
        "for i, v in enumerate(counts):\n",
        "    ax.text(v + max(counts) * 0.01, i, f'{v:,}', va='center', fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIGURES_DIR / 'top_words_analysis.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identify financial keywords and phrases\n",
        "financial_keywords = [\n",
        "    'price', 'target', 'stock', 'share', 'earnings', 'revenue', 'profit',\n",
        "    'fda', 'approval', 'merger', 'acquisition', 'dividend', 'ipo',\n",
        "    'analyst', 'rating', 'upgrade', 'downgrade', 'forecast', 'guidance',\n",
        "    'quarter', 'annual', 'report', 'beat', 'miss', 'expectation'\n",
        "]\n",
        "\n",
        "print(\"=== Financial Keywords Analysis ===\")\n",
        "keyword_counts = {}\n",
        "for keyword in financial_keywords:\n",
        "    count = sum(1 for tokens in df['processed_tokens'] if keyword in tokens)\n",
        "    keyword_counts[keyword] = count\n",
        "\n",
        "keyword_df = pd.DataFrame(list(keyword_counts.items()), \n",
        "                         columns=['Keyword', 'Frequency'])\n",
        "keyword_df = keyword_df.sort_values('Frequency', ascending=False)\n",
        "\n",
        "print(f\"\\nKeyword frequencies:\")\n",
        "display(keyword_df)\n",
        "\n",
        "# Visualization\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "ax.barh(range(len(keyword_df)), keyword_df['Frequency'], color='coral')\n",
        "ax.set_yticks(range(len(keyword_df)))\n",
        "ax.set_yticklabels(keyword_df['Keyword'])\n",
        "ax.set_xlabel('Frequency')\n",
        "ax.set_title('Financial Keywords Frequency in Headlines')\n",
        "ax.invert_yaxis()\n",
        "ax.grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIGURES_DIR / 'financial_keywords.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nâœ… Text analysis complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Time Series Analysis - Publication Frequency\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Resample to different time frequencies\n",
        "df_time = df.set_index('date')\n",
        "\n",
        "# Daily frequency\n",
        "daily_freq = df_time.resample('D').size()\n",
        "\n",
        "# Weekly frequency\n",
        "weekly_freq = df_time.resample('W').size()\n",
        "\n",
        "# Monthly frequency\n",
        "monthly_freq = df_time.resample('M').size()\n",
        "\n",
        "print(\"=== Publication Frequency Statistics ===\")\n",
        "print(f\"\\nDaily Statistics:\")\n",
        "print(f\"  Mean articles per day: {daily_freq.mean():.2f}\")\n",
        "print(f\"  Median articles per day: {daily_freq.median():.2f}\")\n",
        "print(f\"  Max articles in a day: {daily_freq.max()}\")\n",
        "print(f\"  Min articles in a day: {daily_freq.min()}\")\n",
        "print(f\"  Std deviation: {daily_freq.std():.2f}\")\n",
        "\n",
        "print(f\"\\nWeekly Statistics:\")\n",
        "print(f\"  Mean articles per week: {weekly_freq.mean():.2f}\")\n",
        "print(f\"  Median articles per week: {weekly_freq.median():.2f}\")\n",
        "print(f\"  Max articles in a week: {weekly_freq.max()}\")\n",
        "\n",
        "print(f\"\\nMonthly Statistics:\")\n",
        "print(f\"  Mean articles per month: {monthly_freq.mean():.2f}\")\n",
        "print(f\"  Median articles per month: {monthly_freq.median():.2f}\")\n",
        "print(f\"  Max articles in a month: {monthly_freq.max()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identify spikes in publication frequency\n",
        "daily_freq_df = daily_freq.reset_index()\n",
        "daily_freq_df.columns = ['date', 'count']\n",
        "\n",
        "# Calculate z-scores to identify outliers\n",
        "mean_daily = daily_freq.mean()\n",
        "std_daily = daily_freq.std()\n",
        "daily_freq_df['z_score'] = (daily_freq_df['count'] - mean_daily) / std_daily\n",
        "\n",
        "# Days with unusually high publication (z-score > 2)\n",
        "spike_days = daily_freq_df[daily_freq_df['z_score'] > 2].sort_values('count', ascending=False)\n",
        "\n",
        "print(\"=== Days with Unusually High Publication Frequency (Z-score > 2) ===\")\n",
        "print(f\"Number of spike days: {len(spike_days)}\")\n",
        "if len(spike_days) > 0:\n",
        "    print(\"\\nTop 10 spike days:\")\n",
        "    display(spike_days.head(10)[['date', 'count', 'z_score']])\n",
        "\n",
        "# Visualize time series with spikes highlighted\n",
        "fig, axes = plt.subplots(2, 1, figsize=(16, 10))\n",
        "\n",
        "# Daily frequency plot\n",
        "axes[0].plot(daily_freq.index, daily_freq.values, linewidth=1, color='steelblue', alpha=0.7)\n",
        "axes[0].axhline(mean_daily, color='red', linestyle='--', label=f'Mean: {mean_daily:.1f}')\n",
        "axes[0].axhline(mean_daily + 2*std_daily, color='orange', linestyle='--', \n",
        "                label=f'Mean + 2Ïƒ: {mean_daily + 2*std_daily:.1f}')\n",
        "if len(spike_days) > 0:\n",
        "    spike_dates = spike_days['date'].values\n",
        "    spike_counts = spike_days['count'].values\n",
        "    axes[0].scatter(spike_dates, spike_counts, color='red', s=50, zorder=5, \n",
        "                    label='Spikes (z>2)')\n",
        "axes[0].set_xlabel('Date')\n",
        "axes[0].set_ylabel('Number of Articles')\n",
        "axes[0].set_title('Daily Publication Frequency Over Time (with Spike Detection)')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "axes[0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Weekly frequency plot\n",
        "axes[1].plot(weekly_freq.index, weekly_freq.values, linewidth=2, color='coral', marker='o', markersize=3)\n",
        "axes[1].set_xlabel('Date')\n",
        "axes[1].set_ylabel('Number of Articles')\n",
        "axes[1].set_title('Weekly Publication Frequency Over Time')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "axes[1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIGURES_DIR / 'publication_frequency_timeseries.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nâœ… Time series analysis complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Publisher Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze publisher domains (if publisher contains email addresses)\n",
        "def extract_domain(publisher):\n",
        "    \"\"\"Extract domain from publisher field (if it's an email)\"\"\"\n",
        "    if pd.isna(publisher):\n",
        "        return None\n",
        "    publisher_str = str(publisher).lower()\n",
        "    if '@' in publisher_str:\n",
        "        return publisher_str.split('@')[1]\n",
        "    return publisher_str\n",
        "\n",
        "df['publisher_domain'] = df['publisher'].apply(extract_domain)\n",
        "\n",
        "# Count by domain\n",
        "domain_counts = df['publisher_domain'].value_counts()\n",
        "\n",
        "print(\"=== Publisher Domain Analysis ===\")\n",
        "print(f\"Total unique domains/publishers: {len(domain_counts)}\")\n",
        "print(f\"\\nTop 20 Domains/Publishers:\")\n",
        "display(domain_counts.head(20))\n",
        "\n",
        "# Calculate concentration metrics\n",
        "top_10_pct = (domain_counts.head(10).sum() / len(df) * 100).round(2)\n",
        "top_20_pct = (domain_counts.head(20).sum() / len(df) * 100).round(2)\n",
        "\n",
        "print(f\"\\nConcentration Metrics:\")\n",
        "print(f\"  Top 10 publishers account for {top_10_pct}% of articles\")\n",
        "print(f\"  Top 20 publishers account for {top_20_pct}% of articles\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze publisher activity over time\n",
        "top_publishers_list = publisher_counts.head(10).index.tolist()\n",
        "\n",
        "# Monthly activity by top publishers\n",
        "publisher_monthly = df[df['publisher'].isin(top_publishers_list)].groupby(\n",
        "    ['publisher', 'year', 'month']\n",
        ").size().reset_index(name='count')\n",
        "\n",
        "# Create visualization\n",
        "fig, ax = plt.subplots(figsize=(16, 8))\n",
        "\n",
        "for publisher in top_publishers_list[:5]:  # Top 5 for clarity\n",
        "    pub_data = publisher_monthly[publisher_monthly['publisher'] == publisher]\n",
        "    pub_data['date'] = pd.to_datetime(\n",
        "        pub_data[['year', 'month']].assign(day=1)\n",
        "    )\n",
        "    ax.plot(pub_data['date'], pub_data['count'], marker='o', label=publisher, linewidth=2)\n",
        "\n",
        "ax.set_xlabel('Date')\n",
        "ax.set_ylabel('Number of Articles')\n",
        "ax.set_title('Monthly Publication Activity of Top 5 Publishers Over Time')\n",
        "ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "ax.grid(True, alpha=0.3)\n",
        "ax.tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIGURES_DIR / 'publisher_activity_timeseries.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nâœ… Publisher analysis complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Stock Symbol Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze stock symbols\n",
        "if 'stock' in df.columns:\n",
        "    stock_counts = df['stock'].value_counts()\n",
        "    \n",
        "    print(\"=== Stock Symbol Analysis ===\")\n",
        "    print(f\"Total unique stocks: {df['stock'].nunique()}\")\n",
        "    print(f\"\\nTop 20 Most Mentioned Stocks:\")\n",
        "    display(stock_counts.head(20))\n",
        "    \n",
        "    # Visualization\n",
        "    top_20_stocks = stock_counts.head(20)\n",
        "    \n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "    ax.barh(range(len(top_20_stocks)), top_20_stocks.values, color='mediumseagreen')\n",
        "    ax.set_yticks(range(len(top_20_stocks)))\n",
        "    ax.set_yticklabels(top_20_stocks.index)\n",
        "    ax.set_xlabel('Number of Articles')\n",
        "    ax.set_title('Top 20 Stocks by Article Count')\n",
        "    ax.invert_yaxis()\n",
        "    ax.grid(True, alpha=0.3, axis='x')\n",
        "    \n",
        "    # Add value labels\n",
        "    for i, v in enumerate(top_20_stocks.values):\n",
        "        ax.text(v + max(top_20_stocks.values) * 0.01, i, f'{v:,}', \n",
        "                va='center', fontsize=9)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(FIGURES_DIR / 'stock_symbol_analysis.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\nâœ… Stock symbol analysis complete!\")\n",
        "else:\n",
        "    print(\"âš ï¸  'stock' column not found in dataset\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Summary and Key Insights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"EDA SUMMARY - KEY INSIGHTS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(f\"\\nðŸ“Š Dataset Overview:\")\n",
        "print(f\"  â€¢ Total articles: {len(df):,}\")\n",
        "print(f\"  â€¢ Date range: {df['date'].min().date()} to {df['date'].max().date()}\")\n",
        "print(f\"  â€¢ Unique publishers: {df['publisher'].nunique()}\")\n",
        "if 'stock' in df.columns:\n",
        "    print(f\"  â€¢ Unique stocks: {df['stock'].nunique()}\")\n",
        "\n",
        "print(f\"\\nðŸ“ Headline Statistics:\")\n",
        "print(f\"  â€¢ Average headline length: {df['headline_length'].mean():.1f} characters\")\n",
        "print(f\"  â€¢ Average word count: {df['headline_word_count'].mean():.1f} words\")\n",
        "\n",
        "print(f\"\\nðŸ“° Publisher Insights:\")\n",
        "print(f\"  â€¢ Top publisher: {publisher_counts.index[0]} ({publisher_counts.iloc[0]:,} articles)\")\n",
        "print(f\"  â€¢ Top 10 publishers account for {top_10_pct}% of all articles\")\n",
        "\n",
        "print(f\"\\nâ° Temporal Patterns:\")\n",
        "print(f\"  â€¢ Most active day of week: {dow_counts.index[0]} ({dow_counts.iloc[0]:,} articles)\")\n",
        "print(f\"  â€¢ Most active hour: {hourly_counts.index[0]}:00 UTC ({hourly_counts.iloc[0]:,} articles)\")\n",
        "print(f\"  â€¢ Average articles per day: {daily_freq.mean():.2f}\")\n",
        "\n",
        "print(f\"\\nðŸ”¤ Text Analysis:\")\n",
        "print(f\"  â€¢ Total unique words: {len(word_freq):,}\")\n",
        "print(f\"  â€¢ Most common word: '{top_words[0][0]}' ({top_words[0][1]:,} occurrences)\")\n",
        "\n",
        "if len(spike_days) > 0:\n",
        "    print(f\"\\nðŸ“ˆ Publication Spikes:\")\n",
        "    print(f\"  â€¢ Days with unusually high publication: {len(spike_days)}\")\n",
        "    print(f\"  â€¢ Highest spike: {spike_days.iloc[0]['date'].date()} ({spike_days.iloc[0]['count']} articles)\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"âœ… EDA Analysis Complete!\")\n",
        "print(\"=\" * 60)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
