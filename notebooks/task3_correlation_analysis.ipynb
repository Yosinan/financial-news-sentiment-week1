{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 3: Correlation between News Sentiment and Stock Movement\n",
        "\n",
        "**Financial News Sentiment Analysis - Week 1 Challenge**\n",
        "\n",
        "This notebook performs correlation analysis between news sentiment and stock price movements:\n",
        "- **Date Alignment**: Align news and stock datasets by dates\n",
        "- **Sentiment Analysis**: Analyze sentiment of news headlines using NLP tools\n",
        "- **Stock Returns**: Calculate daily stock returns\n",
        "- **Correlation Analysis**: Measure correlation between sentiment scores and stock returns\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard library imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "from datetime import datetime, timedelta\n",
        "from scipy import stats\n",
        "from scipy.stats import pearsonr, spearmanr\n",
        "\n",
        "# Sentiment Analysis\n",
        "from textblob import TextBlob\n",
        "import nltk\n",
        "\n",
        "# Stock data\n",
        "import yfinance as yf\n",
        "\n",
        "# Set style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (14, 8)\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set paths\n",
        "PROJECT_ROOT = Path('..')\n",
        "DATA_DIR = PROJECT_ROOT / 'data'\n",
        "FIGURES_DIR = PROJECT_ROOT / 'figures'\n",
        "\n",
        "# Create directories if they don't exist\n",
        "FIGURES_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# Download NLTK data if needed\n",
        "try:\n",
        "    nltk.data.find('vader_lexicon')\n",
        "except LookupError:\n",
        "    nltk.download('vader_lexicon', quiet=True)\n",
        "\n",
        "print(\"‚úÖ Setup complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load News Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the financial news dataset\n",
        "news_data_files = list(DATA_DIR.glob('*.csv')) + list(DATA_DIR.glob('*.json'))\n",
        "\n",
        "if not news_data_files:\n",
        "    print(\"‚ö†Ô∏è  No news data files found in data/ directory.\")\n",
        "    print(\"Please ensure your dataset is placed in the data/ directory.\")\n",
        "    print(\"Expected columns: headline, url, publisher, date, stock\")\n",
        "    news_df = None\n",
        "else:\n",
        "    news_file = news_data_files[0]\n",
        "    print(f\"üì∞ Loading news data from: {news_file.name}\")\n",
        "    \n",
        "    if news_file.suffix == '.csv':\n",
        "        news_df = pd.read_csv(news_file, parse_dates=['date'], low_memory=False)\n",
        "    elif news_file.suffix == '.json':\n",
        "        news_df = pd.read_json(news_file, convert_dates=['date'])\n",
        "    \n",
        "    # Clean data\n",
        "    news_df = news_df.dropna(subset=['headline', 'date', 'stock'])\n",
        "    news_df['date'] = pd.to_datetime(news_df['date'], errors='coerce', utc=True)\n",
        "    \n",
        "    print(f\"‚úÖ News data loaded successfully!\")\n",
        "    print(f\"Shape: {news_df.shape[0]:,} rows √ó {news_df.shape[1]} columns\")\n",
        "    print(f\"Date range: {news_df['date'].min().date()} to {news_df['date'].max().date()}\")\n",
        "    print(f\"Unique stocks: {news_df['stock'].nunique()}\")\n",
        "    print(f\"\\nFirst few rows:\")\n",
        "    display(news_df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Sentiment Analysis on Headlines\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_sentiment_textblob(text):\n",
        "    \"\"\"\n",
        "    Analyze sentiment using TextBlob.\n",
        "    Returns polarity score (-1 to 1) and subjectivity score (0 to 1).\n",
        "    \"\"\"\n",
        "    if pd.isna(text) or text == '':\n",
        "        return 0.0, 0.0\n",
        "    \n",
        "    blob = TextBlob(str(text))\n",
        "    polarity = blob.sentiment.polarity  # -1 (negative) to 1 (positive)\n",
        "    subjectivity = blob.sentiment.subjectivity  # 0 (objective) to 1 (subjective)\n",
        "    \n",
        "    return polarity, subjectivity\n",
        "\n",
        "print(\"üîç Performing sentiment analysis on headlines...\")\n",
        "print(\"This may take a few moments...\\n\")\n",
        "\n",
        "# Apply sentiment analysis\n",
        "sentiment_results = news_df['headline'].apply(analyze_sentiment_textblob)\n",
        "news_df['sentiment_polarity'] = [result[0] for result in sentiment_results]\n",
        "news_df['sentiment_subjectivity'] = [result[1] for result in sentiment_results]\n",
        "\n",
        "# Classify sentiment\n",
        "news_df['sentiment_label'] = news_df['sentiment_polarity'].apply(\n",
        "    lambda x: 'Positive' if x > 0.1 else ('Negative' if x < -0.1 else 'Neutral')\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Sentiment analysis complete!\")\n",
        "print(f\"\\nSentiment distribution:\")\n",
        "print(news_df['sentiment_label'].value_counts())\n",
        "print(f\"\\nSentiment statistics:\")\n",
        "print(news_df['sentiment_polarity'].describe())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize sentiment distribution\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Sentiment label distribution\n",
        "sentiment_counts = news_df['sentiment_label'].value_counts()\n",
        "axes[0].bar(sentiment_counts.index, sentiment_counts.values, \n",
        "            color=['green', 'gray', 'red'], alpha=0.7)\n",
        "axes[0].set_ylabel('Number of Articles')\n",
        "axes[0].set_title('Sentiment Label Distribution')\n",
        "axes[0].grid(True, alpha=0.3, axis='y')\n",
        "for i, v in enumerate(sentiment_counts.values):\n",
        "    axes[0].text(i, v + max(sentiment_counts.values) * 0.01, f'{v:,}', \n",
        "                ha='center', fontsize=10)\n",
        "\n",
        "# Sentiment polarity distribution\n",
        "axes[1].hist(news_df['sentiment_polarity'], bins=50, edgecolor='black', alpha=0.7, color='steelblue')\n",
        "axes[1].axvline(news_df['sentiment_polarity'].mean(), color='red', linestyle='--', \n",
        "               label=f'Mean: {news_df[\"sentiment_polarity\"].mean():.3f}')\n",
        "axes[1].axvline(0, color='black', linestyle='-', linewidth=0.5, label='Neutral')\n",
        "axes[1].set_xlabel('Sentiment Polarity')\n",
        "axes[1].set_ylabel('Frequency')\n",
        "axes[1].set_title('Sentiment Polarity Distribution')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIGURES_DIR / 'sentiment_distribution.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ Sentiment visualization created!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Date Alignment and Aggregation\n",
        "\n",
        "Normalize dates in both news and stock datasets to ensure alignment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Normalize dates: Convert to date only (remove time component)\n",
        "# This ensures news articles match trading days\n",
        "news_df['date_only'] = news_df['date'].dt.date\n",
        "\n",
        "# Get unique stocks from news data\n",
        "stocks_in_news = news_df['stock'].unique().tolist()\n",
        "print(f\"üìä Stocks found in news data: {len(stocks_in_news)}\")\n",
        "print(f\"Stocks: {stocks_in_news[:10]}...\")  # Show first 10\n",
        "\n",
        "# Aggregate sentiment by stock and date\n",
        "# If multiple articles for same stock on same day, calculate average sentiment\n",
        "daily_sentiment = news_df.groupby(['stock', 'date_only']).agg({\n",
        "    'sentiment_polarity': ['mean', 'count'],\n",
        "    'sentiment_subjectivity': 'mean'\n",
        "}).reset_index()\n",
        "\n",
        "# Flatten column names\n",
        "daily_sentiment.columns = ['stock', 'date', 'avg_sentiment', 'article_count', 'avg_subjectivity']\n",
        "daily_sentiment['date'] = pd.to_datetime(daily_sentiment['date'])\n",
        "\n",
        "print(f\"\\n‚úÖ Daily sentiment aggregated!\")\n",
        "print(f\"Total stock-date combinations: {len(daily_sentiment):,}\")\n",
        "print(f\"\\nSample aggregated data:\")\n",
        "display(daily_sentiment.head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Load and Prepare Stock Price Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Determine date range from news data\n",
        "if news_df is not None and len(news_df) > 0:\n",
        "    min_date = news_df['date'].min()\n",
        "    max_date = news_df['date'].max()\n",
        "    # Extend range slightly to ensure we have stock data\n",
        "    start_date = min_date - timedelta(days=5)\n",
        "    end_date = max_date + timedelta(days=5)\n",
        "    \n",
        "    print(f\"üì• Downloading stock data from {start_date.date()} to {end_date.date()}...\")\n",
        "    print(f\"Analyzing {len(stocks_in_news)} stocks...\\n\")\n",
        "    \n",
        "    # Download stock data for all stocks in news\n",
        "    stock_data = {}\n",
        "    for ticker in stocks_in_news[:20]:  # Limit to first 20 stocks for performance\n",
        "        try:\n",
        "            print(f\"Downloading {ticker}...\", end=\" \")\n",
        "            ticker_obj = yf.Ticker(ticker)\n",
        "            df = ticker_obj.history(start=start_date, end=end_date)\n",
        "            \n",
        "            if not df.empty:\n",
        "                df.columns = [col.lower() for col in df.columns]\n",
        "                df.index.name = 'date'\n",
        "                df = df.reset_index()\n",
        "                df['date'] = pd.to_datetime(df['date']).dt.date\n",
        "                df['date'] = pd.to_datetime(df['date'])\n",
        "                \n",
        "                required_cols = ['date', 'open', 'high', 'low', 'close', 'volume']\n",
        "                if all(col in df.columns for col in required_cols):\n",
        "                    stock_data[ticker] = df\n",
        "                    print(f\"‚úÖ {len(df)} records\")\n",
        "                else:\n",
        "                    print(f\"‚ùå Missing columns\")\n",
        "            else:\n",
        "                print(f\"‚ùå No data\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error: {str(e)[:50]}\")\n",
        "    \n",
        "    print(f\"\\n‚úÖ Successfully downloaded data for {len(stock_data)} stocks\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  No news data available. Cannot proceed with correlation analysis.\")\n",
        "    stock_data = {}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Calculate Daily Stock Returns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate daily returns for each stock\n",
        "stock_returns = {}\n",
        "\n",
        "for ticker, df in stock_data.items():\n",
        "    df = df.copy()\n",
        "    df = df.sort_values('date')\n",
        "    \n",
        "    # Calculate daily returns (percentage change)\n",
        "    df['daily_return'] = df['close'].pct_change() * 100  # Convert to percentage\n",
        "    \n",
        "    # Calculate log returns (alternative method)\n",
        "    df['log_return'] = np.log(df['close'] / df['close'].shift(1)) * 100\n",
        "    \n",
        "    # Store only date and returns\n",
        "    stock_returns[ticker] = df[['date', 'daily_return', 'log_return', 'close']].copy()\n",
        "    \n",
        "    print(f\"‚úÖ Calculated returns for {ticker}: {len(df)} days\")\n",
        "\n",
        "print(f\"\\nüìä Daily returns calculated for {len(stock_returns)} stocks\")\n",
        "\n",
        "# Display sample\n",
        "if stock_returns:\n",
        "    sample_ticker = list(stock_returns.keys())[0]\n",
        "    print(f\"\\nSample returns for {sample_ticker}:\")\n",
        "    display(stock_returns[sample_ticker].head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Merge Sentiment and Stock Returns Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Merge sentiment and returns data for each stock\n",
        "merged_data = []\n",
        "\n",
        "for ticker in stock_returns.keys():\n",
        "    if ticker in daily_sentiment['stock'].values:\n",
        "        # Get sentiment data for this stock\n",
        "        stock_sentiment = daily_sentiment[daily_sentiment['stock'] == ticker].copy()\n",
        "        \n",
        "        # Get returns data for this stock\n",
        "        stock_ret = stock_returns[ticker].copy()\n",
        "        stock_ret['date_only'] = pd.to_datetime(stock_ret['date']).dt.date\n",
        "        stock_ret['date_only'] = pd.to_datetime(stock_ret['date_only'])\n",
        "        \n",
        "        # Merge on date\n",
        "        merged = pd.merge(\n",
        "            stock_sentiment[['date', 'avg_sentiment', 'article_count', 'avg_subjectivity']],\n",
        "            stock_ret[['date', 'daily_return', 'log_return', 'close']],\n",
        "            left_on='date',\n",
        "            right_on='date',\n",
        "            how='inner'\n",
        "        )\n",
        "        \n",
        "        merged['stock'] = ticker\n",
        "        merged_data.append(merged)\n",
        "\n",
        "if merged_data:\n",
        "    correlation_df = pd.concat(merged_data, ignore_index=True)\n",
        "    correlation_df = correlation_df.sort_values(['stock', 'date'])\n",
        "    \n",
        "    print(f\"‚úÖ Merged data created!\")\n",
        "    print(f\"Total merged records: {len(correlation_df):,}\")\n",
        "    print(f\"Stocks with merged data: {correlation_df['stock'].nunique()}\")\n",
        "    print(f\"\\nSample merged data:\")\n",
        "    display(correlation_df.head(10))\n",
        "    \n",
        "    print(f\"\\nData quality:\")\n",
        "    print(f\"  Records with sentiment and returns: {len(correlation_df):,}\")\n",
        "    print(f\"  Missing sentiment: {correlation_df['avg_sentiment'].isna().sum()}\")\n",
        "    print(f\"  Missing returns: {correlation_df['daily_return'].isna().sum()}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  No merged data available. Check stock ticker matching.\")\n",
        "    correlation_df = pd.DataFrame()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Correlation Analysis\n",
        "\n",
        "Calculate correlation between sentiment scores and stock returns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Remove rows with missing data\n",
        "correlation_clean = correlation_df.dropna(subset=['avg_sentiment', 'daily_return'])\n",
        "\n",
        "if len(correlation_clean) > 0:\n",
        "    # Overall correlation (all stocks combined)\n",
        "    overall_corr_pearson, overall_p_pearson = pearsonr(\n",
        "        correlation_clean['avg_sentiment'], \n",
        "        correlation_clean['daily_return']\n",
        "    )\n",
        "    overall_corr_spearman, overall_p_spearman = spearmanr(\n",
        "        correlation_clean['avg_sentiment'], \n",
        "        correlation_clean['daily_return']\n",
        "    )\n",
        "    \n",
        "    print(\"=\" * 70)\n",
        "    print(\"OVERALL CORRELATION ANALYSIS (All Stocks Combined)\")\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"\\nPearson Correlation: {overall_corr_pearson:.4f}\")\n",
        "    print(f\"  P-value: {overall_p_pearson:.4f}\")\n",
        "    print(f\"  Significance: {'***' if overall_p_pearson < 0.001 else '**' if overall_p_pearson < 0.01 else '*' if overall_p_pearson < 0.05 else 'ns'}\")\n",
        "    \n",
        "    print(f\"\\nSpearman Correlation: {overall_corr_spearman:.4f}\")\n",
        "    print(f\"  P-value: {overall_p_spearman:.4f}\")\n",
        "    print(f\"  Significance: {'***' if overall_p_spearman < 0.001 else '**' if overall_p_spearman < 0.01 else '*' if overall_p_spearman < 0.05 else 'ns'}\")\n",
        "    \n",
        "    print(f\"\\nInterpretation:\")\n",
        "    if abs(overall_corr_pearson) < 0.1:\n",
        "        strength = \"negligible\"\n",
        "    elif abs(overall_corr_pearson) < 0.3:\n",
        "        strength = \"weak\"\n",
        "    elif abs(overall_corr_pearson) < 0.5:\n",
        "        strength = \"moderate\"\n",
        "    elif abs(overall_corr_pearson) < 0.7:\n",
        "        strength = \"strong\"\n",
        "    else:\n",
        "        strength = \"very strong\"\n",
        "    \n",
        "    direction = \"positive\" if overall_corr_pearson > 0 else \"negative\"\n",
        "    print(f\"  {strength.capitalize()} {direction} correlation between news sentiment and stock returns\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  No clean data available for correlation analysis\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Per-stock correlation analysis\n",
        "if len(correlation_clean) > 0:\n",
        "    stock_correlations = []\n",
        "    \n",
        "    for ticker in correlation_clean['stock'].unique():\n",
        "        stock_data = correlation_clean[correlation_clean['stock'] == ticker]\n",
        "        \n",
        "        if len(stock_data) > 10:  # Need sufficient data points\n",
        "            corr_pearson, p_pearson = pearsonr(\n",
        "                stock_data['avg_sentiment'], \n",
        "                stock_data['daily_return']\n",
        "            )\n",
        "            corr_spearman, p_spearman = spearmanr(\n",
        "                stock_data['avg_sentiment'], \n",
        "                stock_data['daily_return']\n",
        "            )\n",
        "            \n",
        "            stock_correlations.append({\n",
        "                'Stock': ticker,\n",
        "                'Pearson_Correlation': corr_pearson,\n",
        "                'Pearson_P_Value': p_pearson,\n",
        "                'Spearman_Correlation': corr_spearman,\n",
        "                'Spearman_P_Value': p_spearman,\n",
        "                'Data_Points': len(stock_data),\n",
        "                'Significant': 'Yes' if p_pearson < 0.05 else 'No'\n",
        "            })\n",
        "    \n",
        "    if stock_correlations:\n",
        "        corr_summary = pd.DataFrame(stock_correlations)\n",
        "        corr_summary = corr_summary.sort_values('Pearson_Correlation', ascending=False)\n",
        "        \n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "        print(\"PER-STOCK CORRELATION ANALYSIS\")\n",
        "        print(\"=\" * 70)\n",
        "        display(corr_summary)\n",
        "        \n",
        "        print(f\"\\nüìä Summary Statistics:\")\n",
        "        print(f\"  Mean Pearson Correlation: {corr_summary['Pearson_Correlation'].mean():.4f}\")\n",
        "        print(f\"  Median Pearson Correlation: {corr_summary['Pearson_Correlation'].median():.4f}\")\n",
        "        print(f\"  Stocks with significant correlation (p<0.05): {corr_summary['Significant'].value_counts().get('Yes', 0)}\")\n",
        "        print(f\"  Stocks with positive correlation: {(corr_summary['Pearson_Correlation'] > 0).sum()}\")\n",
        "        print(f\"  Stocks with negative correlation: {(corr_summary['Pearson_Correlation'] < 0).sum()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Visualizations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scatter plot: Sentiment vs Returns\n",
        "if len(correlation_clean) > 0:\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "    \n",
        "    # 1. Overall scatter plot\n",
        "    ax1 = axes[0, 0]\n",
        "    ax1.scatter(correlation_clean['avg_sentiment'], correlation_clean['daily_return'], \n",
        "               alpha=0.5, s=20, color='steelblue')\n",
        "    \n",
        "    # Add regression line\n",
        "    z = np.polyfit(correlation_clean['avg_sentiment'], correlation_clean['daily_return'], 1)\n",
        "    p = np.poly1d(z)\n",
        "    ax1.plot(correlation_clean['avg_sentiment'], p(correlation_clean['avg_sentiment']), \n",
        "            \"r--\", alpha=0.8, linewidth=2, label=f'Linear fit (r={overall_corr_pearson:.3f})')\n",
        "    \n",
        "    ax1.axhline(y=0, color='black', linestyle='-', linewidth=0.5, alpha=0.3)\n",
        "    ax1.axvline(x=0, color='black', linestyle='-', linewidth=0.5, alpha=0.3)\n",
        "    ax1.set_xlabel('Average Sentiment Polarity', fontsize=12)\n",
        "    ax1.set_ylabel('Daily Return (%)', fontsize=12)\n",
        "    ax1.set_title('Sentiment vs Stock Returns (All Stocks)', fontsize=14, fontweight='bold')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 2. Correlation by stock (bar chart)\n",
        "    if 'corr_summary' in locals() and len(corr_summary) > 0:\n",
        "        ax2 = axes[0, 1]\n",
        "        top_stocks = corr_summary.head(15)\n",
        "        colors = ['green' if x > 0 else 'red' for x in top_stocks['Pearson_Correlation']]\n",
        "        ax2.barh(range(len(top_stocks)), top_stocks['Pearson_Correlation'], color=colors, alpha=0.7)\n",
        "        ax2.set_yticks(range(len(top_stocks)))\n",
        "        ax2.set_yticklabels(top_stocks['Stock'])\n",
        "        ax2.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
        "        ax2.set_xlabel('Pearson Correlation Coefficient', fontsize=12)\n",
        "        ax2.set_title('Correlation by Stock (Top 15)', fontsize=14, fontweight='bold')\n",
        "        ax2.invert_yaxis()\n",
        "        ax2.grid(True, alpha=0.3, axis='x')\n",
        "    \n",
        "    # 3. Time series: Sentiment and Returns for top correlated stock\n",
        "    if 'corr_summary' in locals() and len(corr_summary) > 0:\n",
        "        top_stock = corr_summary.iloc[0]['Stock']\n",
        "        top_stock_data = correlation_clean[correlation_clean['stock'] == top_stock].sort_values('date')\n",
        "        \n",
        "        ax3 = axes[1, 0]\n",
        "        ax3_twin = ax3.twinx()\n",
        "        \n",
        "        line1 = ax3.plot(top_stock_data['date'], top_stock_data['avg_sentiment'], \n",
        "                        color='blue', label='Sentiment', linewidth=2)\n",
        "        line2 = ax3_twin.plot(top_stock_data['date'], top_stock_data['daily_return'], \n",
        "                             color='red', label='Daily Return (%)', linewidth=1.5, alpha=0.7)\n",
        "        \n",
        "        ax3.axhline(y=0, color='gray', linestyle='--', linewidth=0.5, alpha=0.5)\n",
        "        ax3_twin.axhline(y=0, color='gray', linestyle='--', linewidth=0.5, alpha=0.5)\n",
        "        \n",
        "        ax3.set_xlabel('Date', fontsize=12)\n",
        "        ax3.set_ylabel('Sentiment Polarity', fontsize=12, color='blue')\n",
        "        ax3_twin.set_ylabel('Daily Return (%)', fontsize=12, color='red')\n",
        "        ax3.set_title(f'{top_stock} - Sentiment and Returns Over Time', fontsize=14, fontweight='bold')\n",
        "        \n",
        "        lines = line1 + line2\n",
        "        labels = [l.get_label() for l in lines]\n",
        "        ax3.legend(lines, labels, loc='best')\n",
        "        ax3.grid(True, alpha=0.3)\n",
        "        ax3.tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # 4. Distribution of correlations\n",
        "    if 'corr_summary' in locals() and len(corr_summary) > 0:\n",
        "        ax4 = axes[1, 1]\n",
        "        ax4.hist(corr_summary['Pearson_Correlation'], bins=20, edgecolor='black', alpha=0.7, color='steelblue')\n",
        "        ax4.axvline(corr_summary['Pearson_Correlation'].mean(), color='red', linestyle='--', \n",
        "                   label=f'Mean: {corr_summary[\"Pearson_Correlation\"].mean():.3f}')\n",
        "        ax4.axvline(0, color='black', linestyle='-', linewidth=0.5)\n",
        "        ax4.set_xlabel('Pearson Correlation Coefficient', fontsize=12)\n",
        "        ax4.set_ylabel('Frequency', fontsize=12)\n",
        "        ax4.set_title('Distribution of Stock Correlations', fontsize=14, fontweight='bold')\n",
        "        ax4.legend()\n",
        "        ax4.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(FIGURES_DIR / 'correlation_analysis.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"‚úÖ Correlation visualizations created!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze correlation with different lags\n",
        "if len(correlation_clean) > 0:\n",
        "    lag_correlations = []\n",
        "    \n",
        "    for lag in range(-2, 3):  # -2 to +2 days\n",
        "        lag_data = correlation_clean.copy()\n",
        "        \n",
        "        if lag != 0:\n",
        "            # Shift returns by lag days\n",
        "            lag_data = lag_data.sort_values(['stock', 'date'])\n",
        "            lag_data['daily_return_lag'] = lag_data.groupby('stock')['daily_return'].shift(-lag)\n",
        "            lag_data_clean = lag_data.dropna(subset=['avg_sentiment', 'daily_return_lag'])\n",
        "            \n",
        "            if len(lag_data_clean) > 10:\n",
        "                corr, p_val = pearsonr(lag_data_clean['avg_sentiment'], lag_data_clean['daily_return_lag'])\n",
        "                lag_correlations.append({\n",
        "                    'Lag': lag,\n",
        "                    'Correlation': corr,\n",
        "                    'P_Value': p_val,\n",
        "                    'Data_Points': len(lag_data_clean)\n",
        "                })\n",
        "        else:\n",
        "            # Already calculated (lag 0)\n",
        "            lag_correlations.append({\n",
        "                'Lag': 0,\n",
        "                'Correlation': overall_corr_pearson,\n",
        "                'P_Value': overall_p_pearson,\n",
        "                'Data_Points': len(correlation_clean)\n",
        "            })\n",
        "    \n",
        "    if lag_correlations:\n",
        "        lag_df = pd.DataFrame(lag_correlations)\n",
        "        lag_df = lag_df.sort_values('Lag')\n",
        "        \n",
        "        print(\"=\" * 70)\n",
        "        print(\"LAG ANALYSIS: Sentiment vs Returns at Different Time Lags\")\n",
        "        print(\"=\" * 70)\n",
        "        print(\"Lag: Negative = sentiment leads returns, Positive = returns lead sentiment\")\n",
        "        print()\n",
        "        display(lag_df)\n",
        "        \n",
        "        # Visualization\n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "        colors = ['green' if x > 0 else 'red' for x in lag_df['Correlation']]\n",
        "        ax.bar(lag_df['Lag'], lag_df['Correlation'], color=colors, alpha=0.7, edgecolor='black')\n",
        "        ax.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
        "        ax.set_xlabel('Lag (days)', fontsize=12)\n",
        "        ax.set_ylabel('Correlation Coefficient', fontsize=12)\n",
        "        ax.set_title('Correlation at Different Time Lags', fontsize=14, fontweight='bold')\n",
        "        ax.grid(True, alpha=0.3, axis='y')\n",
        "        \n",
        "        # Add value labels\n",
        "        for i, row in lag_df.iterrows():\n",
        "            ax.text(row['Lag'], row['Correlation'] + (0.01 if row['Correlation'] > 0 else -0.01), \n",
        "                   f\"{row['Correlation']:.3f}\", ha='center', fontsize=9)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.savefig(FIGURES_DIR / 'lag_analysis.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "        \n",
        "        print(\"\\n‚úÖ Lag analysis complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 70)\n",
        "print(\"CORRELATION ANALYSIS SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "if len(correlation_clean) > 0:\n",
        "    print(f\"\\nüìä Dataset Overview:\")\n",
        "    print(f\"  ‚Ä¢ Total news articles analyzed: {len(news_df):,}\")\n",
        "    print(f\"  ‚Ä¢ Unique stocks: {correlation_df['stock'].nunique() if 'correlation_df' in locals() else 0}\")\n",
        "    print(f\"  ‚Ä¢ Stock-date pairs with both sentiment and returns: {len(correlation_clean):,}\")\n",
        "    \n",
        "    print(f\"\\nüìà Overall Correlation:\")\n",
        "    print(f\"  ‚Ä¢ Pearson Correlation: {overall_corr_pearson:.4f} (p={overall_p_pearson:.4f})\")\n",
        "    print(f\"  ‚Ä¢ Spearman Correlation: {overall_corr_spearman:.4f} (p={overall_p_spearman:.4f})\")\n",
        "    \n",
        "    if 'corr_summary' in locals() and len(corr_summary) > 0:\n",
        "        print(f\"\\nüìä Per-Stock Analysis:\")\n",
        "        print(f\"  ‚Ä¢ Mean correlation: {corr_summary['Pearson_Correlation'].mean():.4f}\")\n",
        "        print(f\"  ‚Ä¢ Stocks with significant correlation: {corr_summary['Significant'].value_counts().get('Yes', 0)}\")\n",
        "        print(f\"  ‚Ä¢ Strongest positive correlation: {corr_summary.iloc[0]['Stock']} ({corr_summary.iloc[0]['Pearson_Correlation']:.4f})\")\n",
        "        if corr_summary['Pearson_Correlation'].min() < 0:\n",
        "            print(f\"  ‚Ä¢ Strongest negative correlation: {corr_summary.iloc[-1]['Stock']} ({corr_summary.iloc[-1]['Pearson_Correlation']:.4f})\")\n",
        "    \n",
        "    if 'lag_df' in locals() and len(lag_df) > 0:\n",
        "        best_lag = lag_df.loc[lag_df['Correlation'].abs().idxmax()]\n",
        "        print(f\"\\n‚è∞ Lag Analysis:\")\n",
        "        print(f\"  ‚Ä¢ Best correlation at lag {best_lag['Lag']} days: {best_lag['Correlation']:.4f}\")\n",
        "    \n",
        "    print(f\"\\nüí° Key Insights:\")\n",
        "    if abs(overall_corr_pearson) < 0.1:\n",
        "        print(f\"  ‚Ä¢ Very weak correlation suggests news sentiment may not be a strong predictor\")\n",
        "    elif abs(overall_corr_pearson) < 0.3:\n",
        "        print(f\"  ‚Ä¢ Weak correlation - sentiment has limited predictive power\")\n",
        "    elif abs(overall_corr_pearson) < 0.5:\n",
        "        print(f\"  ‚Ä¢ Moderate correlation - sentiment can be a useful indicator\")\n",
        "    else:\n",
        "        print(f\"  ‚Ä¢ Strong correlation - sentiment is a significant predictor of returns\")\n",
        "    \n",
        "    if overall_corr_pearson > 0:\n",
        "        print(f\"  ‚Ä¢ Positive correlation: Positive news sentiment associated with positive returns\")\n",
        "    else:\n",
        "        print(f\"  ‚Ä¢ Negative correlation: Positive news sentiment associated with negative returns (contrarian effect)\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"‚úÖ Correlation Analysis Complete!\")\n",
        "print(\"=\" * 70)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
